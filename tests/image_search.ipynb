{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/Users/gkreder/bioimageio-chatbot/.vscode/.env\")\n",
    "from bioimageio_chatbot.chatbot import create_customer_service, QuestionWithHistory, UserProfile\n",
    "from schema_agents.schema import Message\n",
    "import asyncio\n",
    "import yaml\n",
    "import pytest\n",
    "from tqdm.auto import tqdm\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from bioimageio_chatbot.knowledge_base import create_vector_knowledge_base\n",
    "from bioimageio_chatbot.chatbot import load_model_info\n",
    "from bioimageio_chatbot.image_processor import ImageProcessor\n",
    "import requests\n",
    "import re\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioimageio_chatbot.utils import get_manifest, download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [02:57<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = load_model_info()\n",
    "db_path = '/Users/gkreder/Downloads/image_db'\n",
    "yaml_dir = os.path.join(db_path, 'rdf_sources')\n",
    "input_images_dir = os.path.join(db_path, \"input_images\")\n",
    "embeddings_dir = os.path.join(db_path, \"embeddings\")\n",
    "for d in [yaml_dir, input_images_dir, embeddings_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "all_metadata = []\n",
    "all_embeddings = []\n",
    "image_embedder = ImageProcessor()\n",
    "for i_m, m in enumerate(tqdm(models)):\n",
    "    response = requests.get(m['rdf_source'])\n",
    "    if response.status_code == 200:\n",
    "        rdf_source_content = response.content.decode('utf-8')\n",
    "        rdf_dict = yaml.safe_load(rdf_source_content)\n",
    "        try:\n",
    "            nickname = rdf_dict['config']['bioimageio']['nickname']\n",
    "            test_inputs = rdf_dict['test_inputs']\n",
    "            input_axes = rdf_dict['inputs'][0]['axes']\n",
    "        except:\n",
    "            continue\n",
    "        with open(os.path.join(yaml_dir, f\"{nickname}.yaml\"), 'w') as f:\n",
    "            f.write(rdf_source_content)\n",
    "        mislabeled_axes = {'impartial-shark' : 'yx'}\n",
    "        if nickname in mislabeled_axes:\n",
    "            input_axes = mislabeled_axes[nickname]\n",
    "        for i_ti, ti in enumerate(test_inputs):\n",
    "            if \"input_test_time_for\" in ti: # nice-peacock\n",
    "                continue\n",
    "            response = requests.get(ti)\n",
    "            if response.status_code == 200:\n",
    "                input_image_file = os.path.join(input_images_dir, f\"{nickname}.npy\")\n",
    "                with open(input_image_file, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "            embedding = image_embedder.embed_image(input_image_file, input_axes)\n",
    "            np.save(os.path.join(embeddings_dir, f\"{nickname}.npy\"), embedding)\n",
    "            all_metadata.append(rdf_dict)\n",
    "            all_embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embedding_pairs = list(zip(all_embeddings, [x['config']['bioimageio']['nickname'] for x in all_metadata]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nicknames = [x['config']['bioimageio']['nickname'] for x in all_metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/gkreder/bioimageio-chatbot/tests/image_search.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gkreder/bioimageio-chatbot/tests/image_search.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# vectordb = FAISS.from_embeddings(all_embedding_pairs, all_embeddings, metadatas=all_metadata)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gkreder/bioimageio-chatbot/tests/image_search.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# FAISS.from_embeddings(embedding = all_embeddings, metadatas=all_metadata)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gkreder/bioimageio-chatbot/tests/image_search.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m vectordb \u001b[39m=\u001b[39m FAISS\u001b[39m.\u001b[39;49mfrom_documents(all_nicknames, all_embeddings)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gkreder/bioimageio-chatbot/tests/image_search.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# db = FAISS.from_documents(all_metadata, all_embeddings)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/langchain/schema/vectorstore.py:508\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    501\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_documents\u001b[39m(\n\u001b[1;32m    502\u001b[0m     \u001b[39mcls\u001b[39m: Type[VST],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    506\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VST:\n\u001b[1;32m    507\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return VectorStore initialized from documents and embeddings.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m     texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    509\u001b[0m     metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39mmetadatas, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/langchain/schema/vectorstore.py:508\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    501\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_documents\u001b[39m(\n\u001b[1;32m    502\u001b[0m     \u001b[39mcls\u001b[39m: Type[VST],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    506\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VST:\n\u001b[1;32m    507\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return VectorStore initialized from documents and embeddings.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m     texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39;49mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    509\u001b[0m     metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39mmetadatas, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "# vectordb = FAISS.from_embeddings(all_embedding_pairs, all_embeddings, metadatas=all_metadata)\n",
    "# FAISS.from_embeddings(embedding = all_embeddings, metadatas=all_metadata)\n",
    "vectordb = FAISS.from_documents(all_nicknames, all_embeddings)\n",
    "# db = FAISS.from_documents(all_metadata, all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.335'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 4096)}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x.shape for x in all_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['affable-shark',\n",
       " 'impartial-shrimp',\n",
       " 'hiding-tiger',\n",
       " 'kind-seashell',\n",
       " 'chatty-frog',\n",
       " 'organized-badger',\n",
       " 'willing-hedgehog',\n",
       " 'discreet-rooster',\n",
       " 'humorous-owl',\n",
       " 'loyal-parrot',\n",
       " 'fearless-crab',\n",
       " 'non-judgemental-eagle',\n",
       " 'emotional-cricket',\n",
       " 'powerful-chipmunk',\n",
       " 'shivering-raccoon',\n",
       " 'creative-panda',\n",
       " 'conscientious-seashell',\n",
       " 'powerful-fish',\n",
       " 'hiding-blowfish',\n",
       " 'loyal-squid',\n",
       " 'pioneering-rhino',\n",
       " 'wild-whale',\n",
       " 'laid-back-lobster',\n",
       " 'passionate-t-rex',\n",
       " 'impartial-shark',\n",
       " 'thoughtful-turtle',\n",
       " 'straightforward-crocodile',\n",
       " 'independent-shrimp',\n",
       " 'polite-pig',\n",
       " 'placid-llama',\n",
       " 'naked-microbe',\n",
       " 'determined-chipmunk',\n",
       " 'joyful-deer',\n",
       " 'nice-peacock',\n",
       " 'easy-going-sauropod',\n",
       " 'noisy-fish',\n",
       " 'ambitious-sloth',\n",
       " 'organized-cricket',\n",
       " 'noisy-hedgehog',\n",
       " 'amiable-crocodile',\n",
       " 'modest-octopus',\n",
       " 'ambitious-ant',\n",
       " 'efficient-chipmunk',\n",
       " 'courteous-otter']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.save_local(os.path.join(db_path, 'model-zoo-images.faiss'), index_name=\"model-zoo-images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in load_model_info():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'affable-shark'"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m['nickname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.embeddings import Embeddings\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Embeddings' from 'langchain.embeddings' (/Users/gkreder/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/langchain/embeddings/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/gkreder/bioimageio-chatbot/tests/image_search.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gkreder/bioimageio-chatbot/tests/image_search.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m Embeddings\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Embeddings' from 'langchain.embeddings' (/Users/gkreder/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/langchain/embeddings/__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dnnlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/gkreder/bioimageio-chatbot/tests/image_search.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gkreder/bioimageio-chatbot/tests/image_search.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m zhang \u001b[39m=\u001b[39m pkl\u001b[39m.\u001b[39;49mload(\u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m/Users/gkreder/Downloads/vgg16_zhang_perceptual.pkl\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dnnlib'"
     ]
    }
   ],
   "source": [
    "zhang = pkl.load(open(\"/Users/gkreder/Downloads/vgg16_zhang_perceptual.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gkreder/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/gkreder/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/gkreder/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:48<00:00, 11.5MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/gkreder/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "import lpips\n",
    "loss_fn_vgg = lpips.LPIPS(net='vgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (64x1x4096). Calculated output size: (64x0x2048). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/gkreder/bioimageio-chatbot/tests/image_search.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gkreder/bioimageio-chatbot/tests/image_search.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m t1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(all_embeddings[\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gkreder/bioimageio-chatbot/tests/image_search.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m t2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(all_embeddings[\u001b[39m1\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gkreder/bioimageio-chatbot/tests/image_search.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m loss_fn_vgg(t1, t2)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/lpips/lpips.py:119\u001b[0m, in \u001b[0;36mLPIPS.forward\u001b[0;34m(self, in0, in1, retPerLayer, normalize)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m# v0.0 - original release had a bug, where input was not scaled\u001b[39;00m\n\u001b[1;32m    118\u001b[0m in0_input, in1_input \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaling_layer(in0), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaling_layer(in1)) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mversion\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39m0.1\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m (in0, in1)\n\u001b[0;32m--> 119\u001b[0m outs0, outs1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet\u001b[39m.\u001b[39;49mforward(in0_input), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet\u001b[39m.\u001b[39mforward(in1_input)\n\u001b[1;32m    120\u001b[0m feats0, feats1, diffs \u001b[39m=\u001b[39m {}, {}, {}\n\u001b[1;32m    122\u001b[0m \u001b[39mfor\u001b[39;00m kk \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mL):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/lpips/pretrained_networks.py:123\u001b[0m, in \u001b[0;36mvgg16.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    121\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice1(X)\n\u001b[1;32m    122\u001b[0m h_relu1_2 \u001b[39m=\u001b[39m h\n\u001b[0;32m--> 123\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mslice2(h)\n\u001b[1;32m    124\u001b[0m h_relu2_2 \u001b[39m=\u001b[39m h\n\u001b[1;32m    125\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice3(h)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/torch/nn/modules/pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor):\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    167\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, ceil_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[1;32m    168\u001b[0m                         return_indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_indices)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/torch/_jit_internal.py:488\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    487\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 488\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/torch/nn/functional.py:791\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    790\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[0;32m--> 791\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (64x1x4096). Calculated output size: (64x0x2048). Output size is too small"
     ]
    }
   ],
   "source": [
    "t1 = torch.from_numpy(all_embeddings[0])\n",
    "t2 = torch.from_numpy(all_embeddings[1])\n",
    "loss_fn_vgg(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.from_numpy(all_embeddings[0])\n",
    "t2 = torch.from_numpy(all_embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6623162031173706"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(t1.flatten(), t2.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gkreder/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/gkreder/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/gkreder/opt/miniconda3/envs/novel-designer/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "import lpips\n",
    "import torch\n",
    "loss_fn_vgg = lpips.LPIPS(net='vgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:00<00:00, 595.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "resized_images = []\n",
    "for m in tqdm(all_metadata):\n",
    "    nickname = m['config']['bioimageio']['nickname']\n",
    "    input_axes = m['inputs'][0]['axes']\n",
    "    input_image_path = os.path.join(input_images_dir, f\"{nickname}.npy\")\n",
    "    mislabeled_axes = {'impartial-shark' : 'yx'}\n",
    "    if nickname in mislabeled_axes:\n",
    "        input_axes = mislabeled_axes[nickname]\n",
    "    resized_image = image_embedder.resize_image(input_image_path, input_axes)\n",
    "    transposed_image = np.transpose(resized_image, (0,3,1,2))\n",
    "    preprocessed_image = preprocess_input(transposed_image)\n",
    "    torch_image = torch.from_numpy(preprocessed_image.copy()).to(torch.float32)\n",
    "    resized_images.append(torch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(img1 : torch.Tensor, img2 : torch.Tensor) -> float:\n",
    "    diff = loss_fn_vgg(img1, img2)\n",
    "    diff = diff.detach().numpy()[0][0][0][0]\n",
    "    sim = 1 - diff # Closer to 1 = more similar\n",
    "    return(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6516184508800507"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1 = 2\n",
    "i2 = 7\n",
    "sim = loss_fn_vgg(resized_images[i1], resized_images[i2]).detach().numpy()[0][0][0][0]\n",
    "1 - sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 1400, 3)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imread(\"/Users/gkreder/Downloads/microscopy-fruit-fly-neurosciencenews.jpeg\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import PIL as pillow\n",
    "import cv2\n",
    "\n",
    "image_embedder = ImageProcessor()\n",
    "user_img_path = \"/Users/gkreder/Downloads/microscopy-fruit-fly-neurosciencenews.jpeg\"\n",
    "user_img = cv2.imread(user_img_path)\n",
    "user_img_axes = \"cyx\"\n",
    "user_image_resized = image_embedder.resize_image(user_img, user_img_axes, output_format = \"bcyx\") # https://github.com/richzhang/PerceptualSimilarity\n",
    "user_torch = torch.from_numpy(preprocess_input(user_image_resized).copy()).to(torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:05<00:00,  8.42it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_sim(img1, img2):\n",
    "    s = loss_fn_vgg(img1, img2)\n",
    "    return(s.detach().numpy()[0][0][0][0])\n",
    "\n",
    "sims = [get_sim(user_torch, x) for x in tqdm(resized_images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attachments': {'files': []},\n",
       " 'authors': [{'name': 'A. Wolny'}, {'name': 'V. Bondarenko'}],\n",
       " 'cite': [{'text': 'V. Bondarenko et al. Ex vivo Engineering Uterine Environment for Peri-implantation Mouse Development',\n",
       "   'url': 'https://github.com/kreshuklab/mouse-embryo-seg'}],\n",
       " 'config': {'bioimageio': {'created': '2023-03-27 12:11:52.357154',\n",
       "   'doi': '10.5281/zenodo.7774490',\n",
       "   'nickname': 'powerful-fish',\n",
       "   'nickname_icon': '🐟',\n",
       "   'owners': [66700],\n",
       "   'status': 'accepted',\n",
       "   'version_id': '7774490',\n",
       "   'version_name': 'version 4'},\n",
       "  'deepimagej': {'allow_tiling': True,\n",
       "   'model_keys': None,\n",
       "   'prediction': {'postprocess': [{'spec': None}],\n",
       "    'preprocess': [{'spec': None}]},\n",
       "   'pyramidal_model': False,\n",
       "   'test_information': {'inputs': [{'name': 'test_input.npy',\n",
       "      'pixel_size': {'x': 1.0, 'y': 1.0, 'z': 1.0},\n",
       "      'size': '128 x 128 x 32 x 1'}],\n",
       "    'memory_peak': None,\n",
       "    'outputs': [{'name': 'test_output.npy',\n",
       "      'size': '128 x 128 x 32 x 1',\n",
       "      'type': 'image'}],\n",
       "    'runtime': None}}},\n",
       " 'covers': ['https://zenodo.org/api/records/7774490/files/raw.png/content',\n",
       "  'https://zenodo.org/api/records/7774490/files/pred.png/content'],\n",
       " 'description': 'A 3D U-Net trained to predict the cell boundaries in live light sheet images of developing mouse embryo. Voxel size: 0.2×0.2×1.0 µm^3',\n",
       " 'documentation': 'https://zenodo.org/api/records/7774490/files/unet3d.md/content',\n",
       " 'format_version': '0.4.9',\n",
       " 'id': '10.5281/zenodo.6384845/7774490',\n",
       " 'inputs': [{'axes': 'bczyx',\n",
       "   'data_range': [-inf, inf],\n",
       "   'data_type': 'float32',\n",
       "   'name': 'raw',\n",
       "   'shape': [1, 1, 32, 128, 128]}],\n",
       " 'license': 'MIT',\n",
       " 'links': ['deepimagej/deepimagej', 'imjoy/BioImageIO-Packager'],\n",
       " 'maintainers': [{'github_user': 'wolny'}],\n",
       " 'name': '3D UNet Mouse Embryo Live',\n",
       " 'outputs': [{'axes': 'bczyx',\n",
       "   'data_range': [0.0, 1.0],\n",
       "   'data_type': 'float32',\n",
       "   'halo': [0, 0, 8, 16, 16],\n",
       "   'name': 'output0',\n",
       "   'shape': {'offset': [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "    'reference_tensor': 'raw',\n",
       "    'scale': [1.0, 1.0, 1.0, 1.0, 1.0]}}],\n",
       " 'rdf_source': 'https://bioimage-io.github.io/collection-bioimage-io/rdfs/10.5281/zenodo.6384845/7774490/rdf.yaml',\n",
       " 'sample_inputs': ['https://zenodo.org/api/records/7774490/files/sample_input_0.tif/content'],\n",
       " 'sample_outputs': ['https://zenodo.org/api/records/7774490/files/sample_output_0.tif/content'],\n",
       " 'tags': ['3d',\n",
       "  'torchscript',\n",
       "  'ilastik',\n",
       "  'unet',\n",
       "  'live',\n",
       "  'plantseg',\n",
       "  'mouse-embryo',\n",
       "  'cell-membrane',\n",
       "  'animal',\n",
       "  'tissue',\n",
       "  'light-sheet-microscopy',\n",
       "  'semantic-segmentation',\n",
       "  'deepimagej',\n",
       "  'pytorch'],\n",
       " 'test_inputs': ['https://zenodo.org/api/records/7774490/files/test_input.npy/content'],\n",
       " 'test_outputs': ['https://zenodo.org/api/records/7774490/files/test_output.npy/content'],\n",
       " 'timestamp': '2022-11-18T22:04:18.132270',\n",
       " 'type': 'model',\n",
       " 'weights': {'pytorch_state_dict': {'architecture': 'https://zenodo.org/api/records/7774490/files/unet.py/content:UNet3D',\n",
       "   'architecture_sha256': '2da9f7149e144b63fee715fb8fe5154e96bf9aa230d176c91e923d528ef0c4e1',\n",
       "   'kwargs': {'f_maps': [32, 64, 128, 256, 512],\n",
       "    'final_sigmoid': True,\n",
       "    'in_channels': 1,\n",
       "    'is_segmentation': True,\n",
       "    'layer_order': 'gcr',\n",
       "    'num_groups': 8,\n",
       "    'out_channels': 1,\n",
       "    'testing': True},\n",
       "   'pytorch_version': '1.12.1',\n",
       "   'sha256': 'dc267bebbd34da4a66ae55c15a4e056ae883f0bcc30176e5548625491690baf7',\n",
       "   'source': 'https://zenodo.org/api/records/7774490/files/unet-bce-dice-cell-boundary-311021.pytorch/content'},\n",
       "  'torchscript': {'pytorch_version': '1.13.0',\n",
       "   'sha256': '81543061236a71bed490d0bf2602a58c6e73ccff825ded132e1048a3f04f6976',\n",
       "   'source': 'https://zenodo.org/api/records/7774490/files/torchscript_tracing.pt/content'}}}"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metadata[np.argmin(sims)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(730, 1000, 3)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imread(\"/USers/gkreder/Downloads/nuclear_blue_image.jpg\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "novel-designer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
